services:
  ollama:
    container_name: itbms-ollama-container
    image: ollama/ollama:latest
    volumes:
      - itbms_ollama_data:/root/.ollama
    networks:
      - itbms_network
    # Use 'deploy' for GPU support if available
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    
    # Use entrypoint to run the server and pull a model
    entrypoint:
      - "/bin/bash"
      - "-c"
      - "ollama serve & ollama pull llama3 && wait"
    restart: unless-stopped

  database:
    container_name: itbms-db-container
    image: mysql:latest
    environment:
      MYSQL_ROOT_PASSWORD: rootpass
    networks:
      - itbms_network
    volumes:
      - itbms_mysql_data:/var/lib/mysql
      - ./itbms-database/init.sql:/docker-entrypoint-initdb.d/init.sql
      - ./itbms-database/insert.sql:/app/insert.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 3
  
  backend:
    container_name: itbms-be-container
    build:
      context: ./itbms-backend
      dockerfile: dockerfile
    environment:
      FRONTEND_URL: ${FRONTEND_URL}
      DB_URL: ${DB_URL}
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      MAIL_USER: ${MAIL_USER}
      MAIL_PASSWORD: ${MAIL_PASSWORD}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
    networks:
      - itbms_network
    depends_on:
      database:
        condition: service_healthy
      ollama:
        condition: service_started

  frontend:
    container_name: itbms-fe-container
    build:
      context: ./itbms-frontend
      dockerfile: dockerfile
    ports:
      - "80:80"
    networks:
      - itbms_network
    depends_on:
      - backend

networks:
  itbms_network:

volumes:
  itbms_mysql_data:
  itbms_ollama_data: 